{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26e68c9-ae31-4abd-9d99-4806d1a0a6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Disclaimer\n",
    "\n",
    "This notebook was made using **Azure Databricks** so to use this notebook you must use Azure Databricks or simillar. Refer to the following link: https://azure.microsoft.com/es-es/products/databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to replace the \"REPLACE ME\" text with a Secret Key. \n",
    "# You have to contact me at gaston.orphant@hotmail.com asking for it.\n",
    "\n",
    "SECRET_KEY = \"REPLACE ME\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a42e52-a89f-47cf-baba-c5153313842e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Collecting gradio\n",
      "  Downloading gradio-3.23.0-py3-none-any.whl (15.8 MB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "Requirement already satisfied: markupsafe in /databricks/python3/lib/python3.9/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from gradio) (2.26.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: matplotlib in /databricks/python3/lib/python3.9/site-packages (from gradio) (3.4.3)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.8-cp39-cp39-manylinux_2_28_x86_64.whl (143 kB)\n",
      "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from gradio) (2.11.3)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "Collecting semantic-version\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pillow in /databricks/python3/lib/python3.9/site-packages (from gradio) (8.4.0)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from gradio) (1.3.4)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from gradio) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from gradio) (1.20.3)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /databricks/python3/lib/python3.9/site-packages (from altair>=4.2.0->gradio) (3.2.0)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: entrypoints in /databricks/python3/lib/python3.9/site-packages (from altair>=4.2.0->gradio) (0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->gradio) (3.8.0)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.9/site-packages (from huggingface-hub->gradio) (21.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /databricks/python3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (21.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (58.0.4)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub->gradio) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.9/site-packages (from pandas->gradio) (2021.3)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /databricks/python3/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (3.2)\n",
      "Collecting starlette<0.27.0,>=0.26.1\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.9/site-packages (from httpx->gradio) (2021.10.8)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->gradio) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->gradio) (1.26.7)\n",
      "Requirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.9/site-packages (from uvicorn->gradio) (8.0.3)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4710 sha256=697d2d6994289ee1de58f74b3e2117276cd863854b16ec7fafce7dd331509841\n",
      "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: sniffio, uc-micro-py, typing-extensions, rfc3986, multidict, mdurl, h11, frozenlist, anyio, yarl, tqdm, toolz, starlette, pyyaml, pydantic, markdown-it-py, linkify-it-py, httpcore, async-timeout, aiosignal, websockets, uvicorn, semantic-version, python-multipart, pydub, orjson, mdit-py-plugins, huggingface-hub, httpx, fsspec, ffmpy, fastapi, altair, aiohttp, aiofiles, gradio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-172f1b10-4785-42d4-95c5-ac89dbedd283\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "Successfully installed aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 altair-4.2.2 anyio-3.6.2 async-timeout-4.0.2 fastapi-0.95.0 ffmpy-0.3.0 frozenlist-1.3.3 fsspec-2023.3.0 gradio-3.23.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.3 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 orjson-3.8.8 pydantic-1.10.7 pydub-0.25.1 python-multipart-0.0.6 pyyaml-6.0 rfc3986-1.5.0 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.26.1 toolz-0.12.0 tqdm-4.65.0 typing-extensions-4.5.0 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-10.4 yarl-1.8.2\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Requirement already satisfied: plotly in /databricks/python3/lib/python3.9/site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from plotly) (8.0.1)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install gradio\n",
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0afe413-814e-4053-880c-a91cdf6236ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77ddf1b-340c-46aa-9d5f-5b0b78bbae78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Connecting with Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d079e4-93e8-460d-9d95-54a3fa326ca1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Set the data location and type\n",
    "\n",
    "There are two ways to access Azure Blob storage: **account keys** and shared access signatures (SAS).\n",
    "\n",
    "To get started, we need to set the location and type of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c760fc54-dbf0-4715-8df6-8fdff699c029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"datawarehousegoogle\"\n",
    "storage_account_access_key = SECRET_KEY # Send me an e-mail to gaston.orphant@hotmail.com requesting this key\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.\"+ storage_account_name +\".blob.core.windows.net\",\n",
    "  storage_account_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"wasbs://datasets@datawarehousegoogle.blob.core.windows.net/\"\n",
    "file_type = \"parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51114086-25f5-4c9c-8bb3-64ff28b0f0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read the data\n",
    "\n",
    "Now that we have specified our file metadata, we can create a DataFrame. Notice that we use an *option* to specify that we want to infer the schema from the file. We can also explicitly set this to a particular schema if we have one already.\n",
    "\n",
    "First, let's create a DataFrame in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82af9cac-9b93-4fb6-9fa7-661c0b16f2f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(file_type).option(\"inferSchema\", \"true\").load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b4dd6b3-50be-4850-ae49-ec9ef268d3d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drops rows where main_category is not restaurant\n",
    "restaurants = df.filter(df.main_category == \"food services\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e027e65-e86b-4b82-a3a8-c06d10e39405",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Hashing Strings\n",
    "\n",
    "ALS only accepts numerics inputs so we need to transform some columns from string to numbers.\n",
    "\n",
    "We can do it by hashing the strings or by indexing the strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57975416-8704-4dff-b057-950e766fab04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import the functions library as F to do a hash of user_id\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Hashing these columns so they have integers type\n",
    "restaurants = restaurants.withColumn(\"user_id_hash\", F.hash(restaurants.user_id))\n",
    "\n",
    "restaurants = restaurants.withColumn(\"business_id_hash\", F.hash(restaurants.business_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3aecf9-1d6b-4dcf-a905-e5f17e99d053",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop the columns that we don't need for the recommendation system\n",
    "df_ml = restaurants.drop(\"latitude\", \"longitude\", \"main_category\", \"date\", \"resp\", \"opinion\", \"platform\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce2714f-abf5-4fd2-afaa-2d6b2bec2b5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4729955e-cd92-408d-af84-79fd82687117",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f03c1a-5a46-4dfe-a927-a1e13c665de6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# split into training and testing sets\n",
    "\n",
    "(training, test) = df_ml.randomSplit([.7, .3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ddf98c-03a4-43a3-9ca3-f1b85386a9d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-2943557071618452>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[0;31m# With cold start strategy set to 'drop' we ensure we don't get NaN evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[0;31m# We need to be careful using this strategy because we could be losing a lot of data if there are a lot of NULL values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 4\u001b[0;31m \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id_hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'business_id_hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratingCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoldStartStrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m: name 'ALS' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-2943557071618452>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# With cold start strategy set to 'drop' we ensure we don't get NaN evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We need to be careful using this strategy because we could be losing a lot of data if there are a lot of NULL values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'user_id_hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'business_id_hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratingCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoldStartStrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;31mNameError\u001b[0m: name 'ALS' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'ALS' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# With cold start strategy set to 'drop' we ensure we don't get NaN evaluation metrics\n",
    "# We need to be careful using this strategy because we could be losing a lot of data if there are a lot of NULL values.\n",
    "als = ALS(maxIter=10, rank=50, regParam=0.15, userCol='user_id_hash', itemCol='business_id_hash', ratingCol='rating', coldStartStrategy='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62b1910-552c-4618-b6c5-2f39e90e4c40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-2943557071618453>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the ALS model to the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m: name 'als' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-2943557071618453>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the ALS model to the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n\u001b[0;31mNameError\u001b[0m: name 'als' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'als' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the ALS model to the training set\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b77057-81af-4b85-8420-4dcaf68f8af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-2943557071618454>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model by computing the RMSE on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-2943557071618454>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model by computing the RMSE on the test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mNameError\u001b[0m: name 'model' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'model' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369d685a-51de-4a13-bceb-590ad190f6d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Param Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22a1f7a-ad71-4ec2-919e-3af774da77b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Used to know what the best hyperparameters were. \n",
    "# We skip this because we already done it and replaced the best params before.\n",
    "# If you want you can delete the '#' from all lines of code to re-run this.\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "## Initialize the ALS model\n",
    "\n",
    "#als_model = ALS(maxIter=10, regParam=0.15, userCol='user_id_hash', itemCol='business_id_hash', ratingCol='rating', coldStartStrategy='drop')\n",
    "\n",
    "## Create the parameters grid\n",
    "\n",
    "#params = ParamGridBuilder().addGrid(als_model.regParam, [.01, .05, .1, .15]).addGrid(als_model.rank, [10, 50, 100, 150]).build()\n",
    "\n",
    "## Instantiating crossvalidator estimator\n",
    "\n",
    "#cv = CrossValidator(estimator=als_model, estimatorParamMaps=params, evaluator=evaluator, parallelism=4)\n",
    "#best_model = cv.fit(df_ml)\n",
    "#model = best_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb813f00-1ade-4a18-8c04-cdada1d3bce2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5eb0abb-375e-4d56-83b3-8d47834a390e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"/model/modelALS\"\n",
    "model.write().overwrite().save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6cbcb14-edda-495f-84a2-fd7e4dc4fe7d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61262e5-ad22-4af4-a266-6e1c6eb985a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "\n",
    "path = \"/model/modelALS\"\n",
    "model = ALSModel.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3f62a8a-8cbd-428c-b586-ced4d94fd6fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Recommendation System\n",
    "\n",
    "We are going to recommend some restaurants to an user using his ID and checking his previus reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51094ba2-c187-4383-9b2c-557eff6f36aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to retrieve the user_id hashed\n",
    "\n",
    "def id_hashed(user_id):\n",
    "  return df_ml.where(df_ml.user_id == user_id).take(1)[0]['user_id_hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082ee15e-74d6-449b-8a62-50824383528c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to retrieve the restaurant info\n",
    "\n",
    "def name_retriever(business_id_hash, restaurants):\n",
    "    return (restaurants.where(restaurants.business_id_hash == business_id_hash).take(1)[0]['local_name'], restaurants.where(restaurants.business_id_hash == business_id_hash).take(1)[0]['latitude'], restaurants.where(restaurants.business_id_hash == business_id_hash).take(1)[0]['longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38217b61-6596-4551-8841-eb616e955d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Selecting a Random user for now we are using user_id_hash \n",
    "def random_user():\n",
    "  usr_id = df_ml.select('user_id').orderBy(rand()).limit(1).collect()\n",
    "  my_user = [val.user_id for val in usr_id][0]\n",
    "  return my_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8132818-6e7a-4054-b724-84687559066e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def user_recommendation(my_user):\n",
    "  # Opening the dataframe previusly saved\n",
    "  try:\n",
    "    recommendations = spark.read.table(\"recommendations\")\n",
    "  except:\n",
    "    # make recommendations for all users using the recommendForAllUsers method\n",
    "    # we stablish the number of recommendations to show\n",
    "    num_recs = 5\n",
    "    recommendations = model.recommendForAllUsers(num_recs)\n",
    "    #Saving the dataframe\n",
    "    recommendations.write.format(\"parquet\").saveAsTable(\"recommendations\")\n",
    "  \n",
    "  # hash the id inserted\n",
    "  my_user = id_hashed(my_user)\n",
    "  # get recommendations specifically for the user\n",
    "  recs_for_user = recommendations.where(recommendations.user_id_hash == my_user).take(1)\n",
    "  \n",
    "  string = \"\"  \n",
    "  for ranking, (business_id_hash, rating) in enumerate(recs_for_user[0]['recommendations']):\n",
    "    local_name, latitude, longitude = name_retriever(business_id_hash, restaurants)\n",
    "    string = string + \"Recommendation \"+ str(ranking+1) + \": \" + str(local_name) + \". Coordenates: \" + str(latitude) + \", \" + str(longitude) + \"\\n\"\n",
    "  return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634058cc-e782-4307-8505-c0adcc2cef8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Mapping the recommended business\n",
    "def mapped_coor(my_user):\n",
    "    # Opening the dataframe previusly saved\n",
    "  try:\n",
    "    recommendations = spark.read.table(\"recommendations\")\n",
    "  except:\n",
    "    # make recommendations for all users using the recommendForAllUsers method\n",
    "    # we stablish the number of recommendations to show\n",
    "    num_recs = 5\n",
    "    recommendations = model.recommendForAllUsers(num_recs)\n",
    "    #Saving the dataframe\n",
    "    recommendations.write.format(\"parquet\").saveAsTable(\"recommendations\")\n",
    "  \n",
    "  # hash the id inserted\n",
    "  my_user = id_hashed(my_user)\n",
    "  # get recommendations specifically for the user\n",
    "  recs_for_user = recommendations.where(recommendations.user_id_hash == my_user).take(1)\n",
    "  \n",
    "  names = []\n",
    "  lat = []\n",
    "  lon = []\n",
    "  for ranking, (business_id_hash, rating) in enumerate(recs_for_user[0]['recommendations']):\n",
    "    local_name, latitude, longitude = name_retriever(business_id_hash, restaurants)\n",
    "    names.append(local_name)\n",
    "    lat.append(latitude)\n",
    "    lon.append(longitude)\n",
    "    \n",
    "  fig = go.Figure(go.Scattermapbox(\n",
    "            customdata=names,\n",
    "            lat=lat,\n",
    "            lon=lon,\n",
    "            mode='markers',\n",
    "            marker=go.scattermapbox.Marker(\n",
    "                size=8\n",
    "            ),\n",
    "            hoverinfo=\"text\",\n",
    "            hovertemplate='Local Name: %{customdata}'\n",
    "        ))\n",
    "  \n",
    "  fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        hovermode='closest',\n",
    "        mapbox=dict(\n",
    "            bearing=0,\n",
    "            center=go.layout.mapbox.Center(\n",
    "                lat=lat[0],\n",
    "                lon=lon[0]\n",
    "            ),\n",
    "            pitch=0,\n",
    "            zoom=9\n",
    "        ),\n",
    "    )\n",
    "  \n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12e48bd-9a80-44a0-ae8e-aaf2e56c5fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://766610b71c42034097.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://766610b71c42034097.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div><iframe src=\"https://766610b71c42034097.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: "
     ]
    }
   ],
   "source": [
    "# Demo Interface using gradio\n",
    "import gradio as gr\n",
    "\n",
    "title = str(\"Recommendation System\")\n",
    "\n",
    "with gr.Blocks(title= title) as demo:\n",
    "    text = gr.components.HTML(\"\"\"\n",
    "    <center><h1>Welcome to the Demo for the Restaurants Recommendation System!</h1></center>\n",
    "    \"\"\")\n",
    "    text = gr.components.HTML(\"\"\"\n",
    "    <center><h3>You can use the following button to get a random user id, or you can instead put your own user id </h3>\n",
    "    <h4>(Note: You must be registered on Google Maps or Yelp and have rated at least one restaurant)</h4></center>\n",
    "    \"\"\")\n",
    "    get_random = gr.Button(\"Get a Random User ID!\")\n",
    "    userId = gr.Textbox(label=\"Enter your ID:\")\n",
    "    get_random.click(fn=random_user, inputs=None, outputs=userId)\n",
    "    text = gr.components.HTML(\"\"\"\n",
    "    <center><h3>Use the following button to get five restaurants in your zone.</h3></center>\n",
    "    \"\"\")\n",
    "    get_recommendation_btn = gr.Button(\"Get recommendations!\")\n",
    "    output = gr.Textbox(label=\"You can go to the following restaurants:\")\n",
    "    get_recommendation_btn.click(fn=user_recommendation, inputs=userId, outputs=output)\n",
    "    text = gr.components.HTML(\"\"\"\n",
    "    <center><h3>Use the button bellow to get a map with the location of the restaurants previusly mentioned.</h3></center>\n",
    "    \"\"\")\n",
    "    btn = gr.Button(value=\"Show on map\")\n",
    "    map = gr.Plot().style()\n",
    "    demo.load(mapped_coor, userId, map)\n",
    "    btn.click(mapped_coor, userId, map)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3901899426215728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Recommendation System",
   "notebookOrigID": 1396857123802540,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
